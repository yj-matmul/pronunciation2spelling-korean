# Prounciation to Spelling with Korean Project



## 프로젝트 개요

한국어 **STT/TTS** 모델 연구 및 개발에 이용되는 학습 데이터들은 **철자표기가 아닌 한글 발음으로 표기한 데이터**들이 필요합니다. (물론 철자표기 데이터도 STT/TTS End-to-End 학습에 이용될 수 있지만 성능이 발음으로 표기한 데이터를 학습한 모델보다 좋지 않아 잘 이용되지 않는 것으로 알고 있습니다)



> ##### 철자 표기의 예
>
> - 이 상품은 **1+1** 상품입니다.
> - 해당 이벤트 쿠폰은 **1일 1회** **50,000원** 이상 결제 시 사용할 수 있는 쿠폰입니다.
> - **VIP**룸은 **2달** 전부터 예약 가능하며 최소 **2주** 전에 예약해야 사용 가능합니다.
> - 고기 무게가 **660g**이니 한 근에 가격이 얼마에요?
>
> 
>
> ##### 발음 표기의 예
>
> - 이 상품은 **원플러스원** 상품입니다.
> - 해당 이벤트 쿠폰은 **일일 일회** 제한으로 사용할 수 있는 쿠폰입니다.
> - **브이아이피**룸은 **두달** 전부터 예약 가능하며 최소 **이주** 전에 예약해야 사용 가능합니다.
> - 고기 무게가 **육백 육십 그램**이니 한 근에 가격이 얼마에요?



하지만 대부분의 Text 데이터들은 가독성을 위해 발음으로 표현하기 보단 철자로 표기가 되어 있어 학습 데이터를 수집하는데 더해 **발음으로 변환하는 전처리 비용**이 필요합니다. 이를 **자동화 및 효율화**하기 위해 위의 프로젝트를 진행했습니다.



## 프로젝트 기술 내용

#### 활용한 모델

* Transformer
* ELECTRA



#### 데이터

*  AI Hub의 한국어-영어 번역 말뭉치 데이터
* 모두의 말뭉치
* 그 외 직접 수집한 데이터

데이터는 저작권 관련 이슈가 생길 수 있어 공유가 어렵습니다.
또한 weight는 추후 저장할 공간이 생길 시 업로드 하겠습니다.



#### 학습방식

* Seq2Seq 방식으로 학습 및 예측 모델 구현 
* Transformer 학습은 초기 weight 세팅으로 Scratch 학습 진행
* ELECTRA는 KoELECTRA(https://github.com/monologg/KoELECTRA)의 pretrained weight를 이용해 fine tunning을 진행 (Thanks to monologg)



#### 한국어 Token 구축

- Google의 sentencepiece 
- Huggingface의 WordPiece



#### 코드 참고

* 학습 시 (fine-tunning)

  - **run_electra_small_finetune.py (small 모델 추천)**

  * run_electra_base_finetune.py
  * run_transformer_train.py

  참고: encoder의 구조는 미리 pretrain된 구조물을 가져오기에 변경이 불가능합니다.

* 예측 시 (inference)

  * **run_electra_predict.py**
  * run_transformer_predict.py

* 실제 코드를 돌려보려면 데이터의 규격에 대해 커스터 마이징이 필요합니다!


## 프로젝트 결과

* Transformer는 pretrain 없이 학습을 진행하니 성능이 많이 떨어졌습니다.
* ELECTRA는 ELECTA-small, base 모델 구조에 **pretrain weight를 가지고 fine tunning**을 진행하니 변환하는 성능이 월등히 좋아졌습니다. small과 base 모델 결과 간의 차이는 근소한 차이로 base가 좋았으나 **weight 크기 및 모델 메모리 이용량** 등을 고려하면 small 을 이용하는 것을 추천합니다.
  아래는 ELECTRA-small 모델을 이용한 결과입니다.

>#### 철자 표기 > 발음 표기
>
>* 이 상품은 **1+1** 상품입니다. 
>  * 이 상품은 **원플러스원** 상품입니다.
>* 해당 이벤트 쿠폰은 **1일 1회 50,000원** 이상 결제 시 사용할 수 있는 쿠폰입니다.
>  * 해당 이벤트 쿠폰은 **일일 일회 오만원** 이상 결제 시 사용할 수 있는 쿠폰입니다.
>* **VIP**룸은 **2달** 전부터 예약 가능하며 최소 **2주** 전에 예약해야 사용 가능합니다.
>  * **브이아이피**룸은 **두달** 전부터 예약 가능하며 최소 **이주** 전에 예약해야 사용 가능합니다.
>* 고기 무게가 **660g**이니 한 근에 가격이 얼마에요?
>  * 고기 무게가 **육백 육십 그램**이니 한 근에 가격이 얼마에요?
>
>
>
>#### 발음 표기  > 철자 표기
>
>- 전화번호는 **둘 하나 구 삼에 팔 팔 공 하나** 입니다.
>  - 전화번호는 **2193-8801** 입니다.
>- 현재 대한민국과 북한의 점수는 **일대일**입니다.
>  - 현재 대한민국과 북한의 점수는 1:1입니다.
>- 지금 빼빼로 하나 더 구매하시면 **십퍼센트** 할인 가능합니다.
>  - 지금 빼빼로 하나 더 구매하시면 **10%** 할인 가능합니다.



## 프로젝트 의의 및 한계

* STT/TTS 전용 **학습 데이터 생성에 효율성**을 제공합니다.
* 학습 데이터(발음 표기 데이터, 철자 표기 데이터)를 뒤집음으로써 **2가지 기능**이 가능합니다.
  * 철자 표기 > 발음 표기 (TTS 학습 데이터 구축)
  * 발음 표기 > 철자 표기 (STT 결과 후보정)
* 이전에 보지 못한 데이터에 대해서는 예측력이 떨어집니다. 그만큼 **다양한 경우의 데이터 구성**이 중요합니다!
  * 관련 문장 20~50 문장 정도 있으면 어느 정도 예측에 반영이 됩니다.
* **한국어 Seq2Seq inference에 코드 자료**가 많이 없어(제가 구현할 당시엔 별로 없었습니다ㅠ) NLP 머신러닝 엔지니어와 꿈꾸는 사람들에게 참고 자료가 되길 바랍니다!